{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task C:\n",
    "Implementing 2-layered Convolutional Model trained to predict LouisaML_Reduced Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.io as scio\n",
    "import h5py\n",
    "from keras.utils import np_utils\n",
    "import keras.models as models\n",
    "from keras.layers.core import Reshape,Dense,Dropout,Activation,Flatten\n",
    "from keras.layers.noise import GaussianNoise\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras.regularizers import *\n",
    "from keras.optimizers import *\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.callbacks import Callback\n",
    "from sklearn import metrics\n",
    "import matplotlib\n",
    "matplotlib.use('Qt5Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import random, keras\n",
    "import time\n",
    "import _pickle as cPickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-processing\n",
    "Load Dataset, inspect keys, load dataset as array of miostruct objects and inspect its shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['__globals__', '__header__', '__version__', 'dataset'])\n",
      "(8, 20)\n"
     ]
    }
   ],
   "source": [
    "S=scio.loadmat('LouisaML_Reduced.mat');\n",
    "print(S.keys()) \n",
    "data=S['dataset'] \n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract modulation types(label), sigvalue (A) and snr information (snr) from data, i.e. get rid of elements that we do not need and flatten the indexing. Make sure the dataset is not sparse (filled with zeros) or overlaps (double 1s in label)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label[0][5][514] [1 0 0 0]\n",
      "label.shape (8, 20)\n",
      "label_after_unravel [0 1 0 0]\n",
      "lbl [[1 0 0 0]\n",
      " [1 0 0 0]\n",
      " [1 0 0 0]\n",
      " ..., \n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]] 160000\n",
      "lbl.shape (160000, 4)\n",
      "lbl[5514] [1 0 0 0]\n",
      "A[7][18].shape (1000, 2, 128)\n",
      "(20,)\n",
      "label [[1 0 0 0]\n",
      " [1 0 0 0]\n",
      " [1 0 0 0]\n",
      " ..., \n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]]\n",
      "(4,)\n",
      "A.shape: (160000, 2, 128)\n",
      "snr [[ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " ..., \n",
      " [19]\n",
      " [19]\n",
      " [19]] 160000\n",
      "snr.shape and type (160000, 1) <class 'numpy.ndarray'>\n",
      "snr[15984] [15]\n"
     ]
    }
   ],
   "source": [
    "label=data[:,:]['label']\n",
    "#label_before_unravel has 3 level indexes\n",
    "print('label[0][5][514]',label[0][5][514])\n",
    "print('label.shape',label.shape)\n",
    "#label.ravel() returns contiguous flattened array\n",
    "label=label.ravel()\n",
    "print('label_after_unravel',label[20][999])\n",
    "lbl=np.vstack(label)\n",
    "print('lbl',lbl, len(lbl))\n",
    "print('lbl.shape',lbl.shape)\n",
    "print('lbl[5514]',lbl[5514])\n",
    "\n",
    "A=data[:,:]['A']\n",
    "print('A[7][18].shape', A[7][18].shape)\n",
    "print(A[7].shape)\n",
    "A=A.ravel()\n",
    "A=np.vstack(A)\n",
    "print('label',lbl)\n",
    "print(label[7][2].shape)\n",
    "print(\"A.shape:\",A.shape)\n",
    "\n",
    "snr=data[:,:]['snr']\n",
    "snr=snr.ravel()\n",
    "snr=np.vstack(snr)\n",
    "print('snr',snr, len(snr))\n",
    "print('snr.shape and type',snr.shape, type(snr))\n",
    "print('snr[15984]',snr[15984])\n",
    "snr=snr.tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating labels in dictionary format, i.e. keys in the form of tuple (modulation type, SNR type). Total number of keys should be No. of modulation types * No. of SNR types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys([('64QAM', 19), ('16QAM', 9), ('QPSK', 2), ('BPSK', 6), ('QPSK', 12), ('QPSK', 3), ('64QAM', 1), ('BPSK', 17), ('16QAM', 4), ('64QAM', 0), ('64QAM', 15), ('BPSK', 3), ('16QAM', 1), ('QPSK', 4), ('QPSK', 18), ('64QAM', 16), ('16QAM', 8), ('64QAM', 8), ('16QAM', 14), ('BPSK', 7), ('16QAM', 5), ('BPSK', 8), ('BPSK', 14), ('64QAM', 13), ('QPSK', 9), ('16QAM', 11), ('16QAM', 18), ('16QAM', 6), ('64QAM', 2), ('QPSK', 13), ('QPSK', 5), ('QPSK', 19), ('64QAM', 17), ('QPSK', 6), ('BPSK', 15), ('BPSK', 0), ('BPSK', 12), ('QPSK', 1), ('64QAM', 10), ('64QAM', 7), ('16QAM', 0), ('BPSK', 19), ('64QAM', 4), ('16QAM', 17), ('BPSK', 4), ('BPSK', 13), ('64QAM', 9), ('QPSK', 8), ('64QAM', 3), ('QPSK', 10), ('64QAM', 6), ('64QAM', 12), ('QPSK', 15), ('QPSK', 14), ('16QAM', 19), ('BPSK', 1), ('16QAM', 3), ('BPSK', 10), ('64QAM', 18), ('16QAM', 10), ('16QAM', 16), ('16QAM', 13), ('BPSK', 5), ('16QAM', 2), ('16QAM', 7), ('QPSK', 0), ('BPSK', 16), ('QPSK', 11), ('64QAM', 11), ('QPSK', 17), ('16QAM', 15), ('BPSK', 11), ('BPSK', 18), ('BPSK', 9), ('BPSK', 2), ('64QAM', 14), ('QPSK', 7), ('16QAM', 12), ('64QAM', 5), ('QPSK', 16)]) 80\n"
     ]
    }
   ],
   "source": [
    "n=['BPSK','QPSK','16QAM','64QAM']\n",
    "k=[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19]\n",
    "keys = [(n,k) for k in [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19] for n in ['BPSK','QPSK','16QAM','64QAM']]\n",
    "\n",
    "labelDictionary={}\n",
    "for i in range(len(keys)):\n",
    "    for j in range(1000):\n",
    "        labelDictionary[keys[i]]=A[j]\n",
    "print(labelDictionary.keys(), len(labelDictionary.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Split for Training, Test, Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A.shape[0]: 160000\n",
      "A_validation : [[  4.76589601e-03  -8.39680207e-03  -9.69669676e-04  -1.30235452e-02\n",
      "   -1.42604281e-02   3.30993651e-03   2.06726595e-03  -4.64013275e-03\n",
      "    2.73783868e-03  -3.22163011e-03  -4.93991292e-03  -5.30492112e-03\n",
      "   -3.88759067e-03  -1.35701533e-03  -3.35151521e-03   4.50426370e-03\n",
      "    1.69548192e-03  -2.01290870e-03  -3.23605546e-03   9.53620540e-05\n",
      "   -1.02452811e-05   2.22020748e-03  -5.63387386e-04   5.51849686e-03\n",
      "    1.67515307e-03  -2.02031302e-03  -3.67312348e-03  -5.07519912e-03\n",
      "    1.34373465e-03  -4.66858843e-03   7.89879036e-03  -2.95486122e-03\n",
      "   -1.97536626e-03  -3.73153936e-03  -1.76904549e-02  -1.11928855e-02\n",
      "    5.42259525e-03  -4.79759887e-03  -6.71123337e-03   1.34118527e-02\n",
      "    1.09320711e-02   2.65144179e-03  -4.69330529e-03  -3.19556194e-03\n",
      "   -8.00064210e-03  -4.20395124e-03  -7.22356726e-03   9.47140311e-03\n",
      "    2.06095171e-04  -2.27902868e-04   1.99382043e-03  -5.73761578e-03\n",
      "    6.01205887e-03  -8.51660023e-04   1.82183350e-03   3.12323689e-03\n",
      "    7.59229369e-04   6.50205188e-03   3.98965696e-03   1.23402987e-03\n",
      "    1.34007686e-02   3.12180416e-03   3.43118938e-03   5.17477251e-03\n",
      "    3.65420958e-03  -5.91814959e-03  -9.72248205e-03   7.73818195e-03\n",
      "    4.02091884e-03   3.31685378e-03   4.16732730e-03  -8.48329158e-03\n",
      "   -1.43532808e-04   7.35760901e-03   5.63266300e-03   7.90575812e-04\n",
      "    3.18557088e-03  -8.75445247e-04   2.86711619e-04   1.38913562e-02\n",
      "    3.21011030e-03   5.35365624e-03  -4.20714891e-03   3.86704789e-03\n",
      "    6.79765201e-03  -5.10059377e-03  -4.69038767e-03  -3.46678172e-03\n",
      "   -4.57061863e-03  -6.88672185e-03  -1.34837084e-02  -1.21710951e-02\n",
      "    3.30057205e-03   2.87344941e-03  -6.15747169e-03  -3.43058074e-03\n",
      "    9.14135339e-03   4.72165294e-03  -3.24624875e-03   5.61296535e-03\n",
      "    1.36324658e-03  -7.70496726e-03  -6.21716252e-03  -5.55895877e-03\n",
      "   -1.87902491e-03   1.71625320e-03   2.49629700e-03  -1.15134986e-03\n",
      "    6.74784726e-03   7.37778010e-03   2.26772248e-02   6.74752889e-03\n",
      "    5.04970808e-03   5.90673681e-03   6.64249822e-03   5.12350211e-03\n",
      "   -1.46342924e-02   2.02100289e-03  -4.58368988e-03  -2.15254464e-03\n",
      "   -1.50722011e-02  -1.41993736e-02   2.44945748e-03   5.19727376e-03\n",
      "   -3.16543628e-03  -4.82256843e-03  -4.47000760e-03  -2.65917310e-03]\n",
      " [ -3.57533404e-03  -5.88726304e-03  -4.57527080e-03  -3.65001369e-03\n",
      "   -1.91840206e-03  -2.89371525e-05  -1.06571065e-03   3.70367822e-03\n",
      "    8.18833459e-03  -3.24517420e-03   4.03166456e-03   6.65512006e-03\n",
      "    6.99949690e-04   7.37689734e-04   9.60649560e-04  -1.87994449e-03\n",
      "    5.82837824e-04   2.86100422e-03   4.31901458e-03   7.36808953e-03\n",
      "   -4.65100797e-03   7.28015074e-03   6.45549433e-03  -3.54963946e-03\n",
      "    5.51369121e-03   9.99548670e-04  -1.19909773e-03   3.71267318e-03\n",
      "   -8.15622345e-03  -3.52195067e-03   7.38705432e-03  -9.26646288e-03\n",
      "    3.61687210e-03   4.82837128e-03  -3.86069237e-03  -9.37402852e-03\n",
      "    1.17060421e-02   9.53780529e-03  -5.93302069e-03   5.88421516e-03\n",
      "   -7.21935219e-04  -7.67161805e-04   3.49646122e-04   1.72078556e-03\n",
      "   -2.52744445e-03   7.17828235e-03  -2.14168194e-03   3.14614129e-04\n",
      "    5.92323254e-03  -1.53542207e-03   6.39931731e-04   1.15376582e-03\n",
      "   -2.42305955e-03   2.34118630e-03  -6.81451354e-04   5.20455515e-04\n",
      "   -5.85683977e-03  -3.24603455e-03   3.82378914e-03  -5.88893218e-03\n",
      "   -3.58732387e-03  -9.62062207e-03  -6.83037292e-03  -9.72428617e-03\n",
      "    6.68160192e-03   8.78521555e-03  -2.10963396e-04  -3.99166190e-03\n",
      "   -2.88525560e-03  -4.30228899e-03   1.88145540e-04  -4.97324440e-03\n",
      "   -7.58500202e-03  -5.18530510e-03   1.04802267e-03   8.87239733e-03\n",
      "    3.39437370e-03  -1.60039378e-02  -9.14519098e-03  -3.62202451e-03\n",
      "   -4.39079740e-03   7.39436441e-05  -1.25914729e-02   1.05320012e-02\n",
      "   -1.34641420e-03   6.99092246e-03   1.41283790e-03   1.19854028e-02\n",
      "    1.75817156e-02   4.28777153e-03   5.54703152e-03  -1.42629195e-03\n",
      "    1.20903328e-04   1.19237806e-03   3.34065322e-03  -4.65388719e-03\n",
      "   -6.89759483e-03  -9.52234454e-03   5.80749131e-03   4.29274870e-03\n",
      "   -4.28831390e-03   4.36484589e-03   6.26847661e-03  -7.91304951e-03\n",
      "   -6.66092621e-03   3.39185785e-03   5.23093644e-03  -1.23205866e-02\n",
      "   -9.14520338e-03  -3.70505174e-03  -4.71375087e-03   1.37359606e-04\n",
      "    2.38594249e-03  -2.25520045e-03   4.81146547e-03  -2.35209257e-03\n",
      "    6.19867364e-04   5.62475781e-03  -1.61205596e-03   9.30042071e-03\n",
      "   -4.91606090e-03   3.39154627e-03  -2.54190026e-03   1.06651034e-03\n",
      "    6.87292967e-03   1.28502055e-02   2.52497010e-04  -9.44680382e-03]]\n",
      "Y_validation : [1 0 0 0]\n",
      "A_validation : [[  4.76589601e-03  -8.39680207e-03  -9.69669676e-04  -1.30235452e-02\n",
      "   -1.42604281e-02   3.30993651e-03   2.06726595e-03  -4.64013275e-03\n",
      "    2.73783868e-03  -3.22163011e-03  -4.93991292e-03  -5.30492112e-03\n",
      "   -3.88759067e-03  -1.35701533e-03  -3.35151521e-03   4.50426370e-03\n",
      "    1.69548192e-03  -2.01290870e-03  -3.23605546e-03   9.53620540e-05\n",
      "   -1.02452811e-05   2.22020748e-03  -5.63387386e-04   5.51849686e-03\n",
      "    1.67515307e-03  -2.02031302e-03  -3.67312348e-03  -5.07519912e-03\n",
      "    1.34373465e-03  -4.66858843e-03   7.89879036e-03  -2.95486122e-03\n",
      "   -1.97536626e-03  -3.73153936e-03  -1.76904549e-02  -1.11928855e-02\n",
      "    5.42259525e-03  -4.79759887e-03  -6.71123337e-03   1.34118527e-02\n",
      "    1.09320711e-02   2.65144179e-03  -4.69330529e-03  -3.19556194e-03\n",
      "   -8.00064210e-03  -4.20395124e-03  -7.22356726e-03   9.47140311e-03\n",
      "    2.06095171e-04  -2.27902868e-04   1.99382043e-03  -5.73761578e-03\n",
      "    6.01205887e-03  -8.51660023e-04   1.82183350e-03   3.12323689e-03\n",
      "    7.59229369e-04   6.50205188e-03   3.98965696e-03   1.23402987e-03\n",
      "    1.34007686e-02   3.12180416e-03   3.43118938e-03   5.17477251e-03\n",
      "    3.65420958e-03  -5.91814959e-03  -9.72248205e-03   7.73818195e-03\n",
      "    4.02091884e-03   3.31685378e-03   4.16732730e-03  -8.48329158e-03\n",
      "   -1.43532808e-04   7.35760901e-03   5.63266300e-03   7.90575812e-04\n",
      "    3.18557088e-03  -8.75445247e-04   2.86711619e-04   1.38913562e-02\n",
      "    3.21011030e-03   5.35365624e-03  -4.20714891e-03   3.86704789e-03\n",
      "    6.79765201e-03  -5.10059377e-03  -4.69038767e-03  -3.46678172e-03\n",
      "   -4.57061863e-03  -6.88672185e-03  -1.34837084e-02  -1.21710951e-02\n",
      "    3.30057205e-03   2.87344941e-03  -6.15747169e-03  -3.43058074e-03\n",
      "    9.14135339e-03   4.72165294e-03  -3.24624875e-03   5.61296535e-03\n",
      "    1.36324658e-03  -7.70496726e-03  -6.21716252e-03  -5.55895877e-03\n",
      "   -1.87902491e-03   1.71625320e-03   2.49629700e-03  -1.15134986e-03\n",
      "    6.74784726e-03   7.37778010e-03   2.26772248e-02   6.74752889e-03\n",
      "    5.04970808e-03   5.90673681e-03   6.64249822e-03   5.12350211e-03\n",
      "   -1.46342924e-02   2.02100289e-03  -4.58368988e-03  -2.15254464e-03\n",
      "   -1.50722011e-02  -1.41993736e-02   2.44945748e-03   5.19727376e-03\n",
      "   -3.16543628e-03  -4.82256843e-03  -4.47000760e-03  -2.65917310e-03]\n",
      " [ -3.57533404e-03  -5.88726304e-03  -4.57527080e-03  -3.65001369e-03\n",
      "   -1.91840206e-03  -2.89371525e-05  -1.06571065e-03   3.70367822e-03\n",
      "    8.18833459e-03  -3.24517420e-03   4.03166456e-03   6.65512006e-03\n",
      "    6.99949690e-04   7.37689734e-04   9.60649560e-04  -1.87994449e-03\n",
      "    5.82837824e-04   2.86100422e-03   4.31901458e-03   7.36808953e-03\n",
      "   -4.65100797e-03   7.28015074e-03   6.45549433e-03  -3.54963946e-03\n",
      "    5.51369121e-03   9.99548670e-04  -1.19909773e-03   3.71267318e-03\n",
      "   -8.15622345e-03  -3.52195067e-03   7.38705432e-03  -9.26646288e-03\n",
      "    3.61687210e-03   4.82837128e-03  -3.86069237e-03  -9.37402852e-03\n",
      "    1.17060421e-02   9.53780529e-03  -5.93302069e-03   5.88421516e-03\n",
      "   -7.21935219e-04  -7.67161805e-04   3.49646122e-04   1.72078556e-03\n",
      "   -2.52744445e-03   7.17828235e-03  -2.14168194e-03   3.14614129e-04\n",
      "    5.92323254e-03  -1.53542207e-03   6.39931731e-04   1.15376582e-03\n",
      "   -2.42305955e-03   2.34118630e-03  -6.81451354e-04   5.20455515e-04\n",
      "   -5.85683977e-03  -3.24603455e-03   3.82378914e-03  -5.88893218e-03\n",
      "   -3.58732387e-03  -9.62062207e-03  -6.83037292e-03  -9.72428617e-03\n",
      "    6.68160192e-03   8.78521555e-03  -2.10963396e-04  -3.99166190e-03\n",
      "   -2.88525560e-03  -4.30228899e-03   1.88145540e-04  -4.97324440e-03\n",
      "   -7.58500202e-03  -5.18530510e-03   1.04802267e-03   8.87239733e-03\n",
      "    3.39437370e-03  -1.60039378e-02  -9.14519098e-03  -3.62202451e-03\n",
      "   -4.39079740e-03   7.39436441e-05  -1.25914729e-02   1.05320012e-02\n",
      "   -1.34641420e-03   6.99092246e-03   1.41283790e-03   1.19854028e-02\n",
      "    1.75817156e-02   4.28777153e-03   5.54703152e-03  -1.42629195e-03\n",
      "    1.20903328e-04   1.19237806e-03   3.34065322e-03  -4.65388719e-03\n",
      "   -6.89759483e-03  -9.52234454e-03   5.80749131e-03   4.29274870e-03\n",
      "   -4.28831390e-03   4.36484589e-03   6.26847661e-03  -7.91304951e-03\n",
      "   -6.66092621e-03   3.39185785e-03   5.23093644e-03  -1.23205866e-02\n",
      "   -9.14520338e-03  -3.70505174e-03  -4.71375087e-03   1.37359606e-04\n",
      "    2.38594249e-03  -2.25520045e-03   4.81146547e-03  -2.35209257e-03\n",
      "    6.19867364e-04   5.62475781e-03  -1.61205596e-03   9.30042071e-03\n",
      "   -4.91606090e-03   3.39154627e-03  -2.54190026e-03   1.06651034e-03\n",
      "    6.87292967e-03   1.28502055e-02   2.52497010e-04  -9.44680382e-03]]\n",
      "Y_validation : [ 1.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(2016)\n",
    "n_examples = A.shape[0]\n",
    "print(\"A.shape[0]:\", n_examples)\n",
    "n_train = int(n_examples * 0.6)\n",
    "train_idx = list(np.random.choice(n_examples, size=n_train, replace=False)) #changed array to list\n",
    "n_validation = int(n_examples*0.2)\n",
    "validation_set = list(set(range(0,n_examples))-set(train_idx))\n",
    "validation_idx = list(np.random.choice(validation_set, size=n_validation, replace=False)) #hold-out values for validation\n",
    "test_idx = list(set(range(0,n_examples))- set(train_idx) - set(validation_idx))\n",
    "A_train=np.zeros((96000,2,128),np.float64)\n",
    "Y_train=np.zeros((96000,4),np.float64)\n",
    "A_test=np.zeros((32000,2,128),np.float64)\n",
    "Y_test=np.zeros((32000,4),np.float64)\n",
    "A_validation=np.zeros((32000,2,128),np.float64)\n",
    "Y_validation=np.zeros((32000,4),np.float64)\n",
    "\n",
    "#More Shuffling in place\n",
    "np.random.shuffle(train_idx)\n",
    "np.random.shuffle(test_idx)\n",
    "np.random.shuffle(validation_idx)\n",
    "\n",
    "#Filling in A and Y tensors\n",
    "z=0\n",
    "for p in train_idx:\n",
    "    A_train[z] = A[p]\n",
    "    Y_train[z] = lbl[p]\n",
    "    z=z+1\n",
    "\n",
    "z=0\n",
    "for q in test_idx:\n",
    "    A_test[z] = A[q]\n",
    "    Y_test[z] = lbl[q]\n",
    "    z=z+1\n",
    "\n",
    "z=0\n",
    "for r in validation_idx:\n",
    "    A_validation[z] = A[r]\n",
    "    Y_validation[z] = lbl[r]\n",
    "    z=z+1\n",
    "\n",
    "#For verification, A[last_validation] = A_validation[-1] and lbl[last_validation] = Y_validation[-1] \n",
    "last_validation=validation_idx[-1]\n",
    "print(\"A_validation :\", A[last_validation])\n",
    "print(\"Y_validation :\", lbl[last_validation])\n",
    "print(\"A_validation :\",A_validation[-1])\n",
    "print(\"Y_validation :\", Y_validation[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training in Keras Model\n",
    "Prints out Input Shape of Dataset and set up 2-layered CNN Model with Default Adam Optimizer Values, Dropout Rate = 50%, Number of Epochs = 500, Batch Size=1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A_train.shape, Y_train.shape, in_shp : (96000, 2, 128) (96000, 4) [2, 128]\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "reshape_1 (Reshape)          (None, 1, 2, 128)         0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_1 (ZeroPaddin (None, 1, 2, 132)         0         \n",
      "_________________________________________________________________\n",
      "conv1 (Conv2D)               (None, 256, 2, 130)       1024      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256, 2, 130)       0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_2 (ZeroPaddin (None, 256, 2, 134)       0         \n",
      "_________________________________________________________________\n",
      "conv2 (Conv2D)               (None, 80, 1, 132)        122960    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 80, 1, 132)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 10560)             0         \n",
      "_________________________________________________________________\n",
      "dense1 (Dense)               (None, 256)               2703616   \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense2 (Dense)               (None, 4)                 1028      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "reshape_2 (Reshape)          (None, 4)                 0         \n",
      "=================================================================\n",
      "Total params: 2,828,628\n",
      "Trainable params: 2,828,628\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model set up\n",
      "Training NN\n"
     ]
    }
   ],
   "source": [
    "in_shp = list(A_train.shape[1:])\n",
    "print (\"A_train.shape, Y_train.shape, in_shp :\", A_train.shape,Y_train.shape,in_shp)\n",
    "classes=n #Classes to be predicted, i.e. the Modulation Types\n",
    "#Set up some params\n",
    "adam = adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-8, decay=0)\n",
    "dr = 0.5 # dropout rate (%)\n",
    "epochs=500\n",
    "batch_size= 1024\n",
    "model = models.Sequential()\n",
    "model.add(Reshape((1,2,128),input_shape=in_shp))\n",
    "model.add(ZeroPadding2D((0, 2), data_format='channels_first'))\n",
    "model.add(Convolution2D(256,(1,3), activation=\"relu\", name=\"conv1\", padding=\"valid\", kernel_initializer='glorot_uniform',data_format='channels_first'))\n",
    "model.add(Dropout(dr))\n",
    "model.add(ZeroPadding2D((0, 2), data_format='channels_first'))\n",
    "model.add(Convolution2D(80,(2, 3), activation=\"relu\", name=\"conv2\", padding=\"valid\", kernel_initializer='glorot_uniform',data_format='channels_first'))\n",
    "model.add(Dropout(dr))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu', kernel_initializer='he_normal', name=\"dense1\"))\n",
    "model.add(Dropout(dr))\n",
    "model.add(Dense( len(classes), kernel_initializer='he_normal', name=\"dense2\" ))\n",
    "model.add(Activation('softmax'))\n",
    "model.add(Reshape([len(classes)]))\n",
    "#compile/configure models\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[\"accuracy\"]) \n",
    "model.summary()\n",
    "#t1=time.time()\n",
    "#lapsed_time=t1-t0\n",
    "#print(\"Lapsed time is %0.2f seconds\" %lapsed_time)\n",
    "print(\"Model set up\")\n",
    "print(\"Training NN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that NN is initialized with random weights and shape of weights are correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neural net initialised with weights w0: [[ 0.01964319 -0.02461094  0.07247709 -0.01712509  0.06624751 -0.043476\n",
      "  -0.00558118  0.0234168  -0.01061016  0.05644912 -0.00027969  0.05206758\n",
      "   0.03402125  0.01543937  0.00020496  0.03410891 -0.06648343  0.06306066\n",
      "   0.06809185  0.05163629  0.03959253 -0.02239642  0.0567373   0.05940518\n",
      "  -0.03394366 -0.0336021   0.04147181 -0.0741792  -0.04119142  0.02938319\n",
      "  -0.06608526 -0.00865794 -0.0623279  -0.08099447 -0.06418459 -0.01227389\n",
      "  -0.04108376 -0.06001418  0.01984257 -0.0056485  -0.01141667 -0.02476753\n",
      "  -0.06403825  0.07930967 -0.06448561 -0.06161448 -0.01666175  0.01328807\n",
      "   0.02898153  0.06293677  0.06768407  0.07658558  0.06324585 -0.05386682\n",
      "   0.07946286  0.01518368  0.02786968 -0.00263763  0.07207085 -0.06631574\n",
      "  -0.00284047  0.02374373  0.05867499  0.01666339 -0.03525848 -0.05773621\n",
      "   0.02468436  0.05792414  0.02978832  0.01424483  0.07084759 -0.00069081\n",
      "   0.06827396 -0.02517089  0.04556865  0.00524195 -0.02359899 -0.03497031\n",
      "  -0.06218023 -0.04226693 -0.04176834  0.02043193 -0.03966781  0.01459132\n",
      "   0.0829713  -0.06945716 -0.05599982 -0.04626151 -0.06351341 -0.08547729\n",
      "  -0.05931509  0.02051578 -0.00556757 -0.0137846  -0.03540068 -0.03147442\n",
      "  -0.00093088 -0.07052667  0.07589799 -0.05114843 -0.03467676  0.02038152\n",
      "  -0.01442117  0.07860625  0.07493737 -0.04915988 -0.02746832  0.06300098\n",
      "  -0.0813271   0.02580156  0.00486428  0.06424613 -0.06539664  0.06708731\n",
      "  -0.00905947 -0.00032336 -0.02674793 -0.01044933 -0.05264097  0.02451\n",
      "  -0.00300315 -0.06388753  0.08750026  0.05570401  0.00855116  0.0823736\n",
      "  -0.07842802  0.06368226  0.00330919 -0.06848887  0.02094138 -0.00723346\n",
      "   0.06749552 -0.07488919 -0.04750902 -0.08238108 -0.02489986  0.00064573\n",
      "   0.04032053 -0.0171305  -0.08138334  0.05874395 -0.03994083 -0.08647329\n",
      "  -0.03126992 -0.07094509  0.02730357  0.00946468  0.05245794 -0.00369219\n",
      "   0.04025459 -0.07718123 -0.0382521  -0.02444973 -0.08665624 -0.03804943\n",
      "  -0.02169724 -0.00153768  0.0458947  -0.04406962  0.08789653 -0.05481853\n",
      "  -0.00886818 -0.07708739 -0.01879405 -0.08295732 -0.00273905  0.07311238\n",
      "   0.08222146  0.014736    0.04892124  0.03212345  0.04381204 -0.01936655\n",
      "   0.04196955  0.04750311 -0.03457545 -0.00028768  0.02962264 -0.01855715\n",
      "  -0.02219728  0.04654048 -0.05896544  0.02636426 -0.02554164 -0.03750958\n",
      "  -0.07979815 -0.08219088  0.0442688   0.0656935   0.02821247  0.06552519\n",
      "   0.08149587  0.04900374  0.01563041  0.03689337  0.04390226 -0.07280692\n",
      "  -0.02050634  0.02288431  0.01797289  0.08070409  0.05874845 -0.07059756\n",
      "  -0.08758903  0.00831804 -0.08712621 -0.0430632  -0.03724215 -0.07948403\n",
      "   0.03928594 -0.086534    0.0660373   0.05569148 -0.07752297 -0.00960797\n",
      "   0.0620015   0.02134356  0.06720762 -0.05256031 -0.06150008  0.08768736\n",
      "   0.05372103 -0.05665897  0.02341695  0.0061703   0.0278438   0.06130239\n",
      "   0.03077278  0.07283156  0.0102723  -0.08244994  0.0021207   0.08284718\n",
      "   0.01885635 -0.06728464  0.01705346 -0.06018564 -0.06628137 -0.02277309\n",
      "   0.08069785  0.04812683 -0.07044808 -0.02112352  0.00865756  0.00286368\n",
      "  -0.01437774 -0.06858905  0.0856168  -0.07524128 -0.05579715  0.06663133\n",
      "   0.02912411 -0.07236026 -0.07892947  0.0588398 ]] and w1:[[  6.12242371e-02  -4.00833935e-02  -4.49495278e-02   8.80281925e-02\n",
      "   -5.15974872e-02  -4.12543304e-02  -8.80585760e-02  -7.20130131e-02\n",
      "   -6.67348504e-05   1.57848448e-02   6.21881336e-03   1.10378787e-02\n",
      "    6.38754368e-02   1.69336349e-02  -2.02324986e-02  -3.03789452e-02\n",
      "    5.45623899e-03   8.55900347e-02  -6.10591993e-02   7.94502050e-02\n",
      "    8.15912187e-02  -5.81143089e-02   4.00136709e-02  -2.49437168e-02\n",
      "   -6.63628504e-02   1.35394111e-02   2.34271958e-02  -7.49698952e-02\n",
      "    3.35388631e-03   3.03067416e-02   4.91357893e-02   7.51041472e-02\n",
      "    1.91624612e-02  -5.55646792e-02  -3.86916175e-02   1.62019804e-02\n",
      "   -1.33898482e-02  -4.13763151e-02   8.75757933e-02  -1.34527981e-02\n",
      "   -4.94308136e-02   4.29001451e-03  -2.86470540e-02  -4.43826839e-02\n",
      "   -5.62696643e-02  -2.19178870e-02   8.56433511e-02  -1.94075927e-02\n",
      "   -6.23615235e-02  -2.97492556e-02  -8.56204182e-02  -9.19029117e-04\n",
      "    6.05733991e-02   4.55273241e-02  -7.21893311e-02  -8.80311951e-02\n",
      "   -1.02771372e-02  -6.61094561e-02   4.99717295e-02  -1.34274960e-02\n",
      "    2.84719616e-02   3.95067036e-02  -2.26667076e-02  -8.13099146e-02\n",
      "    2.29767039e-02  -1.54659525e-02   4.19694334e-02  -5.73039800e-03\n",
      "   -5.04910573e-02  -3.48405875e-02   7.83536732e-02   3.62673402e-02\n",
      "    8.12825710e-02   1.18396729e-02  -6.72715604e-02  -7.17391297e-02\n",
      "   -5.79432324e-02   1.78106651e-02   5.74662387e-02  -6.73166737e-02\n",
      "    2.33514160e-02  -4.71489690e-02  -6.84270114e-02   4.64832336e-02\n",
      "    6.73784167e-02   3.19448113e-03  -1.12734213e-02   1.42829046e-02\n",
      "   -4.33632880e-02   4.55855131e-02   4.43666428e-02   1.29693449e-02\n",
      "   -7.98266083e-02  -3.65025438e-02   4.59878594e-02  -4.73283119e-02\n",
      "   -8.57754722e-02   3.59711200e-02  -2.28999332e-02  -6.55298233e-02\n",
      "    6.21841848e-02  -4.97422405e-02   5.17041385e-02   5.31411022e-02\n",
      "   -3.14932205e-02   2.55227089e-04   2.71506608e-04   2.07575932e-02\n",
      "    5.21503240e-02   3.17606032e-02  -8.05430338e-02  -8.37834328e-02\n",
      "   -5.74718341e-02  -1.49457157e-02   4.11559194e-02   4.92237955e-02\n",
      "   -1.83451399e-02  -6.06866330e-02   2.46591270e-02   4.90968227e-02\n",
      "   -9.78109241e-03   7.85929710e-02  -6.88661262e-02  -1.70728713e-02\n",
      "    8.19845945e-02   1.77273750e-02   7.90656805e-02   1.10893697e-02\n",
      "   -7.59951174e-02  -3.21549214e-02   3.89958024e-02   7.27293342e-02\n",
      "    2.34567001e-02   1.96682438e-02   2.71776244e-02  -4.02787328e-03\n",
      "   -2.66421214e-02  -2.58947760e-02   1.76801383e-02  -6.11070469e-02\n",
      "   -1.61655247e-03  -6.27294853e-02  -8.46985728e-02  -6.15639165e-02\n",
      "    5.59067875e-02  -6.03069775e-02  -1.82806104e-02  -5.66218719e-02\n",
      "   -9.68564302e-03  -7.32512027e-03  -6.18105829e-02   7.50238895e-02\n",
      "   -8.56719688e-02   8.61852616e-03   1.60666630e-02  -1.08699799e-02\n",
      "    2.40147337e-02   4.45214510e-02   6.61828518e-02   1.74231827e-04\n",
      "    6.10989481e-02   4.86427695e-02   5.94870299e-02  -1.34353787e-02\n",
      "    1.47979409e-02  -7.10212514e-02   3.15615088e-02  -7.88139179e-02\n",
      "   -4.20498773e-02  -8.67325291e-02   7.18081743e-02   7.91249722e-02\n",
      "    1.14371777e-02   2.91681960e-02  -5.69033474e-02  -1.31754652e-02\n",
      "    4.94955778e-02   1.67522132e-02   8.18921328e-02  -8.57958496e-02\n",
      "    1.78500786e-02   3.48430723e-02   2.10106373e-05   1.47964507e-02\n",
      "    7.44786561e-02  -3.20011303e-02   2.98082307e-02  -3.85155976e-02\n",
      "   -3.82784791e-02   8.09709579e-02  -5.82959019e-02   3.65415588e-02\n",
      "   -2.14252248e-02  -5.26682027e-02  -8.22820142e-02  -7.97130540e-02\n",
      "   -9.59257782e-03  -6.44195825e-02   5.62448651e-02   1.15908012e-02\n",
      "   -4.59270738e-02  -1.19665638e-02  -7.50594288e-02   5.03667742e-02\n",
      "    7.03165382e-02   6.20256364e-02  -3.38371545e-02  -2.37741023e-02\n",
      "    5.68916053e-02   5.01630306e-02   5.79567701e-02   5.24118245e-02\n",
      "   -1.08987316e-02  -5.76751940e-02  -8.62098709e-02   2.63963342e-02\n",
      "    6.28249049e-02  -2.03366950e-02  -2.21141875e-02   7.11473823e-02\n",
      "    5.20436317e-02   4.15083170e-02   4.03304398e-02  -8.78067538e-02\n",
      "    4.16177958e-02  -8.80710259e-02  -1.34776607e-02   4.51892316e-02\n",
      "   -4.25964482e-02  -2.07150728e-02  -1.24505162e-03  -5.00959419e-02\n",
      "   -4.03068624e-02   6.27729148e-02   6.91542029e-02   9.59358364e-03\n",
      "    7.72408694e-02   4.82715666e-04  -4.92235422e-02   7.55249113e-02\n",
      "    5.40029407e-02   7.26833194e-02  -1.71506032e-02   5.42644411e-02\n",
      "   -8.29184800e-03  -4.92693484e-02  -2.51923501e-03  -6.68223873e-02\n",
      "   -6.97985962e-02  -7.19067976e-02  -3.14359926e-02  -8.58804435e-02\n",
      "   -8.34147558e-02   3.21982056e-02  -7.31545910e-02   3.08714584e-02]]\n",
      "Weight shape (1, 256) (1, 256)\n"
     ]
    }
   ],
   "source": [
    "weights = model.get_weights()\n",
    "w0=weights[0][0][0]\n",
    "w1=weights[0][0][1]\n",
    "print(\"neural net initialised with weights w0: {} and w1:{}\".format(w0,w1))\n",
    "print (\"Weight shape\", w0.shape, w1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start training and record total training time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 96000 samples, validate on 32000 samples\n",
      "Epoch 1/500\n",
      "20s - loss: 1.3264 - acc: 0.3727 - val_loss: 1.3233 - val_acc: 0.3763\n",
      "Epoch 2/500\n",
      "19s - loss: 1.3227 - acc: 0.3734 - val_loss: 1.3310 - val_acc: 0.3763\n",
      "Epoch 3/500\n",
      "19s - loss: 1.3202 - acc: 0.3734 - val_loss: 1.3142 - val_acc: 0.3763\n",
      "Epoch 4/500\n",
      "19s - loss: 1.3164 - acc: 0.3737 - val_loss: 1.3136 - val_acc: 0.3763\n",
      "Epoch 5/500\n",
      "19s - loss: 1.3112 - acc: 0.3752 - val_loss: 1.3039 - val_acc: 0.3782\n",
      "Epoch 6/500\n",
      "19s - loss: 1.3048 - acc: 0.3805 - val_loss: 1.2967 - val_acc: 0.3827\n",
      "Epoch 7/500\n",
      "20s - loss: 1.2962 - acc: 0.3850 - val_loss: 1.2889 - val_acc: 0.3855\n",
      "Epoch 8/500\n",
      "19s - loss: 1.2891 - acc: 0.3894 - val_loss: 1.2810 - val_acc: 0.3943\n",
      "Epoch 9/500\n",
      "19s - loss: 1.2786 - acc: 0.3951 - val_loss: 1.2680 - val_acc: 0.4001\n",
      "Epoch 10/500\n",
      "19s - loss: 1.2663 - acc: 0.4043 - val_loss: 1.2558 - val_acc: 0.4224\n",
      "Epoch 11/500\n",
      "19s - loss: 1.2420 - acc: 0.4244 - val_loss: 1.2226 - val_acc: 0.4361\n",
      "Epoch 12/500\n",
      "19s - loss: 1.2053 - acc: 0.4452 - val_loss: 1.1758 - val_acc: 0.4631\n",
      "Epoch 13/500\n",
      "19s - loss: 1.1753 - acc: 0.4647 - val_loss: 1.1312 - val_acc: 0.4993\n",
      "Epoch 14/500\n",
      "19s - loss: 1.1543 - acc: 0.4800 - val_loss: 1.1114 - val_acc: 0.5068\n",
      "Epoch 15/500\n",
      "19s - loss: 1.1298 - acc: 0.4952 - val_loss: 1.0915 - val_acc: 0.5315\n",
      "Epoch 16/500\n",
      "19s - loss: 1.1098 - acc: 0.5070 - val_loss: 1.0770 - val_acc: 0.5411\n",
      "Epoch 17/500\n",
      "19s - loss: 1.0904 - acc: 0.5200 - val_loss: 1.0517 - val_acc: 0.5494\n",
      "Epoch 18/500\n",
      "19s - loss: 1.0745 - acc: 0.5286 - val_loss: 1.0333 - val_acc: 0.5621\n",
      "Epoch 19/500\n",
      "19s - loss: 1.0552 - acc: 0.5389 - val_loss: 1.0169 - val_acc: 0.5727\n",
      "Epoch 20/500\n",
      "19s - loss: 1.0373 - acc: 0.5494 - val_loss: 1.0060 - val_acc: 0.5732\n",
      "Epoch 21/500\n",
      "19s - loss: 1.0213 - acc: 0.5574 - val_loss: 0.9838 - val_acc: 0.5859\n",
      "Epoch 22/500\n",
      "19s - loss: 1.0074 - acc: 0.5656 - val_loss: 0.9701 - val_acc: 0.5929\n",
      "Epoch 23/500\n",
      "19s - loss: 0.9915 - acc: 0.5736 - val_loss: 0.9515 - val_acc: 0.6037\n",
      "Epoch 24/500\n",
      "19s - loss: 0.9736 - acc: 0.5811 - val_loss: 0.9453 - val_acc: 0.6009\n",
      "Epoch 25/500\n",
      "19s - loss: 0.9632 - acc: 0.5868 - val_loss: 0.9330 - val_acc: 0.6074\n",
      "Epoch 26/500\n",
      "19s - loss: 0.9513 - acc: 0.5932 - val_loss: 0.9405 - val_acc: 0.6034\n",
      "Epoch 27/500\n",
      "19s - loss: 0.9375 - acc: 0.5994 - val_loss: 0.9133 - val_acc: 0.6218\n",
      "Epoch 28/500\n",
      "19s - loss: 0.9184 - acc: 0.6092 - val_loss: 0.8980 - val_acc: 0.6244\n",
      "Epoch 29/500\n",
      "19s - loss: 0.9104 - acc: 0.6108 - val_loss: 0.8972 - val_acc: 0.6234\n",
      "Epoch 30/500\n",
      "19s - loss: 0.8956 - acc: 0.6209 - val_loss: 0.8889 - val_acc: 0.6216\n",
      "Epoch 31/500\n",
      "20s - loss: 0.8895 - acc: 0.6207 - val_loss: 0.8797 - val_acc: 0.6293\n",
      "Epoch 32/500\n",
      "19s - loss: 0.8817 - acc: 0.6256 - val_loss: 0.8758 - val_acc: 0.6265\n",
      "Epoch 33/500\n",
      "19s - loss: 0.8694 - acc: 0.6330 - val_loss: 0.8623 - val_acc: 0.6388\n",
      "Epoch 34/500\n",
      "19s - loss: 0.8599 - acc: 0.6363 - val_loss: 0.8543 - val_acc: 0.6414\n",
      "Epoch 35/500\n",
      "19s - loss: 0.8515 - acc: 0.6419 - val_loss: 0.8468 - val_acc: 0.6456\n",
      "Epoch 36/500\n",
      "19s - loss: 0.8430 - acc: 0.6429 - val_loss: 0.8431 - val_acc: 0.6483\n",
      "Epoch 37/500\n",
      "19s - loss: 0.8312 - acc: 0.6510 - val_loss: 0.8355 - val_acc: 0.6500\n",
      "Epoch 38/500\n",
      "19s - loss: 0.8240 - acc: 0.6539 - val_loss: 0.8415 - val_acc: 0.6470\n",
      "Epoch 39/500\n",
      "19s - loss: 0.8146 - acc: 0.6588 - val_loss: 0.8310 - val_acc: 0.6498\n",
      "Epoch 40/500\n",
      "19s - loss: 0.8067 - acc: 0.6602 - val_loss: 0.8212 - val_acc: 0.6552\n",
      "Epoch 41/500\n",
      "19s - loss: 0.8031 - acc: 0.6636 - val_loss: 0.8215 - val_acc: 0.6539\n",
      "Epoch 42/500\n",
      "19s - loss: 0.7871 - acc: 0.6702 - val_loss: 0.8157 - val_acc: 0.6585\n",
      "Epoch 43/500\n",
      "19s - loss: 0.7848 - acc: 0.6727 - val_loss: 0.8111 - val_acc: 0.6589\n",
      "Epoch 44/500\n",
      "19s - loss: 0.7816 - acc: 0.6717 - val_loss: 0.8071 - val_acc: 0.6642\n",
      "Epoch 45/500\n",
      "19s - loss: 0.7647 - acc: 0.6811 - val_loss: 0.8054 - val_acc: 0.6623\n",
      "Epoch 46/500\n",
      "19s - loss: 0.7643 - acc: 0.6810 - val_loss: 0.8031 - val_acc: 0.6594\n",
      "Epoch 47/500\n",
      "19s - loss: 0.7565 - acc: 0.6851 - val_loss: 0.7950 - val_acc: 0.6668\n",
      "Epoch 48/500\n",
      "19s - loss: 0.7496 - acc: 0.6886 - val_loss: 0.7902 - val_acc: 0.6698\n",
      "Epoch 49/500\n",
      "19s - loss: 0.7397 - acc: 0.6924 - val_loss: 0.7870 - val_acc: 0.6704\n",
      "Epoch 50/500\n",
      "19s - loss: 0.7391 - acc: 0.6921 - val_loss: 0.7904 - val_acc: 0.6679\n",
      "Epoch 51/500\n",
      "19s - loss: 0.7280 - acc: 0.6976 - val_loss: 0.7884 - val_acc: 0.6663\n",
      "Epoch 52/500\n",
      "19s - loss: 0.7201 - acc: 0.7007 - val_loss: 0.7778 - val_acc: 0.6741\n",
      "Epoch 53/500\n",
      "19s - loss: 0.7192 - acc: 0.6992 - val_loss: 0.7768 - val_acc: 0.6744\n",
      "Epoch 54/500\n",
      "19s - loss: 0.7104 - acc: 0.7056 - val_loss: 0.7744 - val_acc: 0.6752\n",
      "Epoch 55/500\n",
      "19s - loss: 0.7038 - acc: 0.7084 - val_loss: 0.7839 - val_acc: 0.6715\n",
      "Epoch 56/500\n",
      "19s - loss: 0.6959 - acc: 0.7106 - val_loss: 0.7701 - val_acc: 0.6776\n",
      "Epoch 57/500\n",
      "19s - loss: 0.6900 - acc: 0.7145 - val_loss: 0.7793 - val_acc: 0.6682\n",
      "Epoch 58/500\n",
      "19s - loss: 0.6872 - acc: 0.7168 - val_loss: 0.7700 - val_acc: 0.6736\n",
      "Epoch 59/500\n",
      "19s - loss: 0.6862 - acc: 0.7165 - val_loss: 0.7630 - val_acc: 0.6798\n",
      "Epoch 60/500\n",
      "19s - loss: 0.6754 - acc: 0.7196 - val_loss: 0.7682 - val_acc: 0.6790\n",
      "Epoch 61/500\n",
      "19s - loss: 0.6750 - acc: 0.7221 - val_loss: 0.7616 - val_acc: 0.6819\n",
      "Epoch 62/500\n",
      "19s - loss: 0.6701 - acc: 0.7239 - val_loss: 0.7544 - val_acc: 0.6837\n",
      "Epoch 63/500\n",
      "19s - loss: 0.6648 - acc: 0.7254 - val_loss: 0.7533 - val_acc: 0.6833\n",
      "Epoch 64/500\n",
      "20s - loss: 0.6534 - acc: 0.7322 - val_loss: 0.7578 - val_acc: 0.6835\n",
      "Epoch 65/500\n",
      "19s - loss: 0.6541 - acc: 0.7332 - val_loss: 0.7504 - val_acc: 0.6847\n",
      "Epoch 66/500\n",
      "19s - loss: 0.6454 - acc: 0.7335 - val_loss: 0.7513 - val_acc: 0.6842\n",
      "Epoch 67/500\n",
      "19s - loss: 0.6444 - acc: 0.7359 - val_loss: 0.7512 - val_acc: 0.6857\n",
      "Epoch 68/500\n",
      "20s - loss: 0.6389 - acc: 0.7379 - val_loss: 0.7450 - val_acc: 0.6864\n",
      "Epoch 69/500\n",
      "19s - loss: 0.6358 - acc: 0.7389 - val_loss: 0.7486 - val_acc: 0.6867\n",
      "Epoch 70/500\n",
      "19s - loss: 0.6318 - acc: 0.7413 - val_loss: 0.7435 - val_acc: 0.6880\n",
      "Epoch 71/500\n",
      "19s - loss: 0.6252 - acc: 0.7450 - val_loss: 0.7421 - val_acc: 0.6882\n",
      "Epoch 72/500\n",
      "19s - loss: 0.6262 - acc: 0.7416 - val_loss: 0.7410 - val_acc: 0.6894\n",
      "Epoch 73/500\n",
      "19s - loss: 0.6184 - acc: 0.7468 - val_loss: 0.7427 - val_acc: 0.6907\n",
      "Epoch 74/500\n",
      "19s - loss: 0.6173 - acc: 0.7464 - val_loss: 0.7536 - val_acc: 0.6837\n",
      "Epoch 75/500\n",
      "19s - loss: 0.6133 - acc: 0.7491 - val_loss: 0.7411 - val_acc: 0.6907\n",
      "Epoch 76/500\n",
      "19s - loss: 0.6092 - acc: 0.7499 - val_loss: 0.7408 - val_acc: 0.6883\n",
      "Epoch 77/500\n",
      "19s - loss: 0.6041 - acc: 0.7534 - val_loss: 0.7345 - val_acc: 0.6908\n",
      "Epoch 78/500\n",
      "19s - loss: 0.6031 - acc: 0.7535 - val_loss: 0.7369 - val_acc: 0.6903\n",
      "Epoch 79/500\n",
      "19s - loss: 0.5973 - acc: 0.7564 - val_loss: 0.7380 - val_acc: 0.6905\n",
      "Epoch 80/500\n",
      "19s - loss: 0.5973 - acc: 0.7563 - val_loss: 0.7341 - val_acc: 0.6935\n",
      "Epoch 81/500\n",
      "19s - loss: 0.5904 - acc: 0.7581 - val_loss: 0.7309 - val_acc: 0.6932\n",
      "Epoch 82/500\n",
      "19s - loss: 0.5891 - acc: 0.7593 - val_loss: 0.7447 - val_acc: 0.6866\n",
      "Epoch 83/500\n",
      "19s - loss: 0.5851 - acc: 0.7612 - val_loss: 0.7407 - val_acc: 0.6869\n",
      "Epoch 84/500\n",
      "19s - loss: 0.5808 - acc: 0.7644 - val_loss: 0.7355 - val_acc: 0.6926\n",
      "Epoch 85/500\n",
      "19s - loss: 0.5817 - acc: 0.7608 - val_loss: 0.7364 - val_acc: 0.6936\n",
      "Epoch 86/500\n",
      "19s - loss: 0.5813 - acc: 0.7644 - val_loss: 0.7382 - val_acc: 0.6893\n",
      "Epoch 87/500\n",
      "19s - loss: 0.5725 - acc: 0.7669 - val_loss: 0.7311 - val_acc: 0.6920\n",
      "Total Training Time for NN1:  996.9924629999999  seconds\n"
     ]
    }
   ],
   "source": [
    "t0=time.clock()\n",
    "filepath='NN1_1024_0.5.weights.h5'\n",
    "history = model.fit(A_train,Y_train,batch_size=batch_size,epochs=epochs,verbose=2,validation_data=(A_validation, Y_validation),callbacks =[keras.callbacks.ModelCheckpoint(filepath, monitor='val_loss', verbose=0, save_best_only=True, mode='auto'),keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, verbose=0, mode='auto')])\n",
    "t1=time.clock()\n",
    "print(\"Total Training Time for NN1: \",t1-t0,\" seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print scores [Loss, Accuracy] and plot training accuracy and loss curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.73113847827911382, 0.69199999999999995]\n"
     ]
    }
   ],
   "source": [
    "score=model.evaluate(A_validation, Y_validation, verbose=0, batch_size=batch_size)\n",
    "print(score)\n",
    "#plot accuracy curves\n",
    "plt.figure(1)\n",
    "plt.subplot(211)\n",
    "plt.title('Accuracy plot')\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend(['train','validation'], loc='upper left')\n",
    "\n",
    "#plot loss curves\n",
    "plt.subplot(212)\n",
    "plt.title('Training performance')\n",
    "plt.plot(history.epoch, history.history['loss'], label='train loss+error')\n",
    "plt.plot(history.epoch,history.history['val_loss'],label='val_error')\n",
    "plt.ylabel('Error')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend(['train','validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting with Trained Weights\n",
    "Load weights, start prediction, plot confusion matrix for all SNRs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded weights from Disk\n",
      "31744/32000 [============================>.] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "model.load_weights(filepath)\n",
    "print(\"Loaded weights from Disk\")\n",
    "\n",
    "#defines function which return all indices for value\n",
    "def all_indices(value, qlist):\n",
    "    indices = []\n",
    "    idx = -1\n",
    "    while True:\n",
    "        try:\n",
    "            idx = qlist.index(value, idx+1)\n",
    "            indices.append(idx)\n",
    "        except ValueError:\n",
    "            break\n",
    "    return indices\n",
    "\n",
    "#defines function to plot confusion matrix\n",
    "def plot_confusion_matrix(cm, title, cmap=plt.cm.Blues, labels=[]):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(labels))\n",
    "    plt.xticks(tick_marks, labels, rotation=45)\n",
    "    plt.yticks(tick_marks, labels)\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "    #plt.show\n",
    "    plt.savefig(title,format='png')\n",
    "\n",
    "#Given A_test, predict Y_hat\n",
    "test_Y_hat = model.predict(A_test, batch_size=batch_size, verbose=1)\n",
    "#Initialise zero vectors for confusion matrix ( normalized and before normalization)\n",
    "conf = np.zeros([len(classes),len(classes)])\n",
    "confnorm = np.zeros([len(classes),len(classes)])\n",
    "for m in range(0,A_test.shape[0]):\n",
    "    n = list(Y_test[m,:]).index(1)\n",
    "    o = int(np.argmax(test_Y_hat[m,:]))\n",
    "    conf[n,o] = conf[n,o] + 1\n",
    "for i in range(0,len(classes)):\n",
    "    confnorm[i,:] = conf[i,:] / np.sum(conf[i,:])\n",
    "\n",
    "#Plot the first confusion matrix for all SNRs\n",
    "plot_confusion_matrix(confnorm,title=\"Confusion_Matrix_for_all_SNRs\",labels=classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict and plot Confusion Matrix according to SNRs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/louisa/anaconda3/lib/python3.5/site-packages/matplotlib/pyplot.py:524: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  max_open_warning, RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Accuracy:  0.694670846395\n",
      "Overall Accuracy:  0.774412855377\n",
      "Overall Accuracy:  0.668918918919\n",
      "Overall Accuracy:  0.683076923077\n",
      "Overall Accuracy:  0.653775322284\n",
      "Overall Accuracy:  0.721706864564\n",
      "Overall Accuracy:  0.57087628866\n",
      "Overall Accuracy:  0.658491722869\n",
      "Overall Accuracy:  0.71835443038\n",
      "Overall Accuracy:  0.656015037594\n",
      "Overall Accuracy:  0.684345047923\n",
      "Overall Accuracy:  0.787411689146\n",
      "Overall Accuracy:  0.686261980831\n",
      "Overall Accuracy:  0.630692451653\n",
      "Overall Accuracy:  0.757748260595\n",
      "Overall Accuracy:  0.688649334179\n",
      "Overall Accuracy:  0.744112030554\n",
      "Overall Accuracy:  0.657734470158\n",
      "Overall Accuracy:  0.69302615193\n",
      "Overall Accuracy:  0.723225030084\n",
      "{0: 0.69467084639498433, 1: 0.77441285537700866, 2: 0.66891891891891897, 3: 0.68307692307692303, 4: 0.65377532228360957, 5: 0.72170686456400746, 6: 0.57087628865979378, 7: 0.65849172286940527, 8: 0.71835443037974689, 9: 0.65601503759398494, 10: 0.68434504792332274, 11: 0.78741168914579318, 12: 0.68626198083067091, 13: 0.63069245165315035, 14: 0.75774826059456035, 15: 0.68864933417882057, 16: 0.74411203055378738, 17: 0.65773447015834352, 18: 0.69302615193026151, 19: 0.72322503008423589}\n",
      "a [0.69467084639498433, 0.77441285537700866, 0.66891891891891897, 0.68307692307692303, 0.65377532228360957, 0.72170686456400746, 0.57087628865979378, 0.65849172286940527, 0.71835443037974689, 0.65601503759398494, 0.68434504792332274, 0.78741168914579318, 0.68626198083067091, 0.63069245165315035, 0.75774826059456035, 0.68864933417882057, 0.74411203055378738, 0.65773447015834352, 0.69302615193026151, 0.72322503008423589]\n"
     ]
    }
   ],
   "source": [
    "#Initialise acc and test_SNRs\n",
    "acc = {}\n",
    "test_SNRs=[]\n",
    "\n",
    "#Generate a list of test_SNRs\n",
    "for i in test_idx:\n",
    "    test_SNRs.append(snr[i])\n",
    "\n",
    "#Extract sigvalues and labels @ SNR\n",
    "for snr in k:\n",
    "    test_X_i = A_test[all_indices([snr],test_SNRs)]\n",
    "    test_Y_i = Y_test[all_indices([snr],test_SNRs)]\n",
    "    test_Y_i_hat = model.predict(test_X_i)\n",
    "    conf = np.zeros([len(classes),len(classes)])\n",
    "    confnorm = np.zeros([len(classes),len(classes)])\n",
    "    for j in range(0,test_X_i.shape[0]):\n",
    "        true = list(test_Y_i[j,:]).index([1])\n",
    "        predicted = int(np.argmax(test_Y_i_hat[j,:]))\n",
    "        conf[true,predicted] = conf[true,predicted] + 1\n",
    "    #np.seterr(divide='ignore',invalid='ignore')\n",
    "    for ct in range(0,len(classes)):\n",
    "        confnorm[ct,:] = conf[ct,:] / np.sum(conf[ct,:])\n",
    "    #print(\"conf\",conf)\n",
    "    #print(\"confnorm\",confnorm)\n",
    "    plt.figure()\n",
    "    plot_confusion_matrix(confnorm, labels=classes, title=\"ConvNet Confusion Matrix (SNR=%d)\"%(snr))\n",
    "    \n",
    "    cor = np.sum(np.diag(conf))\n",
    "    ncor = np.sum(conf) - cor\n",
    "    print (\"Overall Accuracy: \", cor / (cor+ncor))\n",
    "    acc[snr] = 1.0*cor/(cor+ncor)\n",
    "# Save results to a pickle file for plotting later\n",
    "print (acc)\n",
    "a=list(map(lambda x:acc[x], k))\n",
    "print(\"a\",a)\n",
    "with open('results_cnn2_d0.5.dat','wb') as fd:\n",
    "    cPickle.dump((\"CNN2_080218\", 0.5, acc),fd)\n",
    "# Plot accuracy curve\n",
    "fig=plt.figure()\n",
    "plt.plot(k, a)\n",
    "plt.xlabel(\"Signal to Noise Ratio\")\n",
    "plt.ylabel(\"Classification Accuracy\")\n",
    "plt.title(\"CNN2 Classification Accuracy on Dataset 7.146\")\n",
    "plt.show()\n",
    "fig.savefig('Accplot.png')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
